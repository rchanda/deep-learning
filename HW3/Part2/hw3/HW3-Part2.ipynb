{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.autograd import Variable\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "from warpctc_pytorch import CTCLoss\n",
    "from ctcdecode import CTCBeamDecoder\n",
    "\n",
    "from phoneme_list import *\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cuda = True\n",
    "pin_memory = use_cuda\n",
    "\n",
    "dev_feats = np.load('../data/dev.npy')\n",
    "dev_labels = np.load('../data/dev_phonemes.npy')\n",
    "\n",
    "train_feats = np.load('../data/train.npy')\n",
    "train_labels = np.load('../data/train_phonemes.npy')\n",
    "\n",
    "test_feats = np.load('../data/test.npy')\n",
    "test_labels = [np.zeros(1) for i in range(test_feats.shape[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpeechDataset(Dataset):\n",
    "    def __init__(self, feats, labels):\n",
    "        self.feats = feats\n",
    "        self.labels = np.asarray([label+1 for label in labels])\n",
    "        self.length = feats.shape[0]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return (self.feats[index], self.labels[index])\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(data):\n",
    "    data = sorted(data, key=lambda x: x[0].shape[0], reverse=True)\n",
    "    feats_batch, labels_batch = zip(*data)\n",
    "    feats_batch = list(feats_batch)\n",
    "    labels_batch = list(labels_batch)\n",
    "    \n",
    "    batch_size = len(feats_batch)\n",
    "\n",
    "    feats_batch_lens = np.zeros(batch_size)\n",
    "    max_feats_batch_len = 0\n",
    "    for i in range(batch_size):\n",
    "        feats_len = feats_batch[i].shape[0]\n",
    "        feats_batch_lens[i] = feats_len\n",
    "        max_feats_batch_len = max(max_feats_batch_len, feats_len)\n",
    "    \n",
    "    labels_batch_lens = np.zeros(batch_size)\n",
    "    labels_batch_concat = []\n",
    "    for i in range(batch_size):\n",
    "        labels = labels_batch[i]\n",
    "        labels_batch_concat.extend(labels)\n",
    "        labels_batch_lens[i] = labels.shape[0]\n",
    "    \n",
    "    feats_batch_padded = []\n",
    "    for feats in feats_batch:\n",
    "        pad = max_feats_batch_len - feats.shape[0]\n",
    "        feats_padded = np.pad(feats, [(0, pad), (0,0)], 'constant')\n",
    "        feats_batch_padded.append(feats_padded)\n",
    "    \n",
    "    labels_batch = torch.from_numpy(np.asarray(labels_batch_concat)).int()\n",
    "    assert(labels_batch.shape[0] == np.sum(labels_batch_lens, axis=0))\n",
    "    labels_batch_lens = torch.from_numpy(labels_batch_lens).int()\n",
    "    \n",
    "    feats_batch_padded = torch.from_numpy(np.asarray(feats_batch_padded)).float()\n",
    "    feats_batch_padded = feats_batch_padded.transpose(1,0)\n",
    "    assert(feats_batch_padded.shape[0] == max_feats_batch_len)\n",
    "    assert(feats_batch_padded.shape[1] == batch_size)\n",
    "    assert(feats_batch_padded.shape[2] == 40)\n",
    "    \n",
    "    feats_batch_lens = torch.from_numpy(feats_batch_lens).int()\n",
    "    \n",
    "    return (feats_batch_padded, feats_batch_lens, labels_batch, labels_batch_lens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RLSTMModel(nn.Module):\n",
    "    def __init__(self, embed_size, hidden_size, out_size, num_layers):\n",
    "        super(RLSTMModel, self).__init__()\n",
    "        self.rnns = nn.ModuleList([\n",
    "            nn.LSTM(input_size=embed_size, hidden_size=hidden_size, num_layers=num_layers, bidirectional=True)\n",
    "        ])\n",
    "        self.linear1 = nn.Linear(2*hidden_size, hidden_size)\n",
    "        self.linear2 = nn.Linear(hidden_size, out_size)\n",
    "        \n",
    "    def forward(self, inputs, lengths):\n",
    "        packed_h = pack_padded_sequence(inputs, lengths)\n",
    "        for rnn in self.rnns:\n",
    "            packed_h, state = rnn(packed_h)\n",
    "        h, _ = pad_packed_sequence(packed_h) \n",
    "        h = self.linear1(h)\n",
    "        h = self.linear2(h)\n",
    "        \n",
    "        return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "num_workers = 4\n",
    "\n",
    "dset_dev = SpeechDataset(dev_feats, dev_labels)\n",
    "dev_loader = DataLoader(dset_dev, shuffle=True, batch_size=batch_size,\n",
    "    collate_fn=collate_fn,\n",
    "    num_workers=num_workers, pin_memory=pin_memory)\n",
    "\n",
    "dset_train = SpeechDataset(train_feats, train_labels)\n",
    "train_loader = DataLoader(dset_train, shuffle=True, batch_size=batch_size,\n",
    "    collate_fn=collate_fn,\n",
    "    num_workers=num_workers, pin_memory=pin_memory)\n",
    "\n",
    "dset_test = SpeechDataset(test_feats, test_labels)\n",
    "test_loader = DataLoader(dset_test, shuffle=False, batch_size=1,\n",
    "    collate_fn=collate_fn,\n",
    "    num_workers=num_workers, pin_memory=pin_memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 15\n",
    "lr = 0.001\n",
    "embed_size = 40\n",
    "hidden_size = 512\n",
    "out_size = 47\n",
    "num_layers = 3\n",
    "\n",
    "model = RLSTMModel(embed_size, hidden_size, out_size, num_layers)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "ctc_loss = CTCLoss()\n",
    "label_map = [' '] + PHONEME_MAP\n",
    "decoder = CTCBeamDecoder(\n",
    "    labels=label_map,\n",
    "    blank_id=0\n",
    "    )\n",
    "\n",
    "if use_cuda:\n",
    "    model = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    utterance_count = 0\n",
    "    \n",
    "    for batch_idx, (feats, feats_lens, labels, labels_lens) in enumerate(train_loader):\n",
    "        if use_cuda:\n",
    "            feats = feats.cuda()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        logits = model(Variable(feats), feats_lens.numpy())\n",
    "        loss = ctc_loss(logits, Variable(labels), Variable(feats_lens), Variable(labels_lens))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.data[0]\n",
    "        utterance_count += feats_lens.shape[0]\n",
    "        \n",
    "        if batch_idx%100 == 0:\n",
    "            print(\"Batch %d Loss %f\" % (batch_idx, train_loss/utterance_count))\n",
    "\n",
    "    print(\"Epoch %d Train Loss %f\" % (epoch, train_loss/utterance_count))\n",
    "    torch.save(model.state_dict(), str(epoch)+'-model.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval(epoch):\n",
    "    model.eval()\n",
    "    dev_loss = 0\n",
    "    \n",
    "    for batch_idx, (feats, feats_lens, labels, labels_lens) in enumerate(dev_loader):\n",
    "        if use_cuda:\n",
    "            feats = feats.cuda()\n",
    "\n",
    "        logits = model(Variable(feats), feats_lens.numpy())\n",
    "        loss = ctc_loss(logits, Variable(labels), Variable(feats_lens), Variable(labels_lens))\n",
    "        dev_loss += loss.data[0]\n",
    "\n",
    "    print(\"Epoch %d Dev Loss %f\" % (epoch, dev_loss/dev_feats.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(epoch):\n",
    "    model.load_state_dict(torch.load(str(epoch)+'-model.pkl'))\n",
    "\n",
    "    f = open('output.txt', 'w')\n",
    "    f.write('Id,Predicted\\n')\n",
    "\n",
    "    for batch_idx, (feats, feats_lens, labels, labels_lens) in enumerate(test_loader):\n",
    "        if use_cuda:\n",
    "            feats = feats.cuda()\n",
    "\n",
    "        logits = model(Variable(feats), feats_lens.numpy())\n",
    "        probs = F.softmax(logits, dim=2).data.cpu()\n",
    "        output, scores, timesteps, out_seq_len = decoder.decode(probs=probs, seq_lens=feature_lengths)\n",
    "        for i in range(output.size(0)):\n",
    "            chrs = \"\".join(label_map[o] for o in output[i, 0, :out_seq_len[i, 0]])\n",
    "            f.write(batch_idx+','+chrs+'\\n')\n",
    "\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0 Loss 22.945805\n",
      "Batch 100 Loss 19.935591\n",
      "Batch 200 Loss 20.308666\n",
      "Batch 300 Loss 20.817042\n",
      "Batch 400 Loss 21.093894\n",
      "Batch 500 Loss 21.300864\n",
      "Batch 600 Loss 21.294295\n",
      "Batch 700 Loss 21.435289\n",
      "Batch 800 Loss 21.525262\n",
      "Batch 900 Loss 21.478372\n",
      "Batch 1000 Loss 21.397946\n",
      "Batch 1100 Loss 21.518361\n",
      "Batch 1200 Loss 21.856545\n",
      "Batch 1300 Loss 21.957307\n",
      "Batch 1400 Loss 22.053671\n",
      "Batch 1500 Loss 22.114248\n",
      "Epoch 8 Train Loss 22.077339\n",
      "46.97245885928472\n",
      "Epoch 8 Dev Loss 27.705164\n",
      "47.99044941663742\n",
      "Batch 0 Loss 20.660088\n",
      "Batch 100 Loss 17.130318\n",
      "Batch 200 Loss 18.015152\n",
      "Batch 300 Loss 18.608673\n",
      "Batch 400 Loss 18.913439\n",
      "Batch 500 Loss 18.884309\n",
      "Batch 600 Loss 19.152796\n",
      "Batch 700 Loss 19.309788\n",
      "Batch 800 Loss 19.362247\n",
      "Batch 900 Loss 19.631692\n",
      "Batch 1000 Loss 19.879479\n",
      "Batch 1100 Loss 20.015452\n",
      "Batch 1200 Loss 20.107218\n",
      "Batch 1300 Loss 20.214188\n",
      "Batch 1400 Loss 20.228809\n",
      "Batch 1500 Loss 20.408028\n",
      "Epoch 9 Train Loss 20.428026\n",
      "46.92634385029475\n",
      "Epoch 9 Dev Loss 27.759174\n",
      "47.94999454418818\n",
      "Batch 0 Loss 12.724422\n"
     ]
    }
   ],
   "source": [
    "epoch = 7\n",
    "model.load_state_dict(torch.load(str(epoch)+'-model.pkl'))\n",
    "\n",
    "for epoch in range(8, num_epochs):\n",
    "    epoch_start_time = time.time()\n",
    "    train(epoch)\n",
    "    print((time.time() - epoch_start_time)/60)\n",
    "    eval(epoch)\n",
    "    print((time.time() - epoch_start_time)/60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_pytorch_p36)",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
