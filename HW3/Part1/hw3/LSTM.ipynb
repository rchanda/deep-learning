{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import random\n",
    "\n",
    "from torch.optim import Adam\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = np.load('../dataset/wiki.train.npy')\n",
    "val_data = np.load('../dataset/wiki.valid.npy')\n",
    "vocab = np.load('../dataset/vocab.npy') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataLoader(DataLoader):\n",
    "    def __init__(self, array, batch_size):\n",
    "        random.shuffle(array)\n",
    "        data = np.concatenate((array))\n",
    "        m = len(data) // batch_size\n",
    "        \n",
    "        data = data[: m*batch_size+1]\n",
    "        self.inputs = data[:-1].reshape(batch_size, m).T\n",
    "        self.labels = data[1:].reshape(batch_size, m).T\n",
    "        \n",
    "        self.inputs = torch.from_numpy(self.inputs).long()\n",
    "        self.labels = torch.from_numpy(self.labels).long()\n",
    "        \n",
    "        if torch.cuda.is_available():\n",
    "            self.inputs = self.inputs.cuda()\n",
    "            self.labels = self.labels.cuda()\n",
    "            \n",
    "    def __iter__(self):\n",
    "        for i in range(self.len):\n",
    "            start = i*self.seq_length\n",
    "            end = (i+1)*self.seq_length\n",
    "\n",
    "            yield (self.inputs[start:end], self.labels[start:end])\n",
    "        \n",
    "    def __len__(self):\n",
    "        # Generate Random Length for each epoch\n",
    "        len1 = np.random.normal(70,5,1)[0]\n",
    "        len2 = np.random.normal(35,5,1)[0]\n",
    "        random_len = np.random.choice([len1, len2], size=1, p=[0.95, 0.05])[0]\n",
    "        \n",
    "        self.seq_length = int(random_len) if random_len > 0 and random_len < 100 else 70\n",
    "        self.len = self.inputs.shape[0] // self.seq_length\n",
    "        \n",
    "        print(\"seq_length\", self.seq_length)\n",
    "        return self.len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 80\n",
    "train_loader = CustomDataLoader(train_data, batch_size)\n",
    "val_loader = CustomDataLoader(val_data, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_gumbel(shape, eps=1e-10, out=None):\n",
    "    \"\"\"\n",
    "    Sample from Gumbel(0, 1)\n",
    "    based on\n",
    "    https://github.com/ericjang/gumbel-softmax/blob/3c8584924603869e90ca74ac20a6a03d99a91ef9/Categorical%20VAE.ipynb ,\n",
    "    (MIT license)\n",
    "    \"\"\"\n",
    "    U = out.resize_(shape).uniform_() if out is not None else torch.rand(shape)\n",
    "    return - torch.log(eps - torch.log(U + eps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RLSTM(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, hidden_size):\n",
    "        super(RLSTM, self).__init__()        \n",
    "        self.encoder = nn.Embedding(vocab_size, embed_size)\n",
    "        self.rnns = nn.ModuleList([\n",
    "            nn.LSTM(input_size=embed_size, hidden_size=hidden_size, batch_first=True),\n",
    "            nn.LSTM(input_size=hidden_size, hidden_size=hidden_size, batch_first=True),\n",
    "            nn.LSTM(input_size=hidden_size, hidden_size=embed_size, batch_first=True)\n",
    "        ])\n",
    "        self.decoder = nn.Linear(embed_size, vocab_size)\n",
    "        \n",
    "        self.decoder.weight = self.encoder.weight\n",
    "\n",
    "    def forward(self, inputs, forward=0, stochastic=False):\n",
    "        h = inputs  # (n, t)\n",
    "        h = self.encoder(h)  # (n, t, c)\n",
    "        states = []\n",
    "        for rnn in self.rnns:\n",
    "            h, state = rnn(h)\n",
    "            states.append(state)\n",
    "        h = self.decoder(h)\n",
    "        if stochastic:\n",
    "            gumbel = Variable(sample_gumbel(shape=h.size(), out=h.data.new()))\n",
    "            h += gumbel\n",
    "        logits = h\n",
    "        \n",
    "        if forward > 0:\n",
    "            outputs = []\n",
    "            print(logits[:, -1:, :].shape)\n",
    "            h = torch.max(logits[:, -1:, :], dim=2)[1]\n",
    "            for i in range(forward):\n",
    "                h = self.encoder(h)\n",
    "                for j, rnn in enumerate(self.rnns):\n",
    "                    h, state = rnn(h, states[j])\n",
    "                    states[j] = state\n",
    "                h = self.decoder(h)\n",
    "                if stochastic:\n",
    "                    gumbel = Variable(sample_gumbel(shape=h.size(), out=h.data.new()))\n",
    "                    h += gumbel\n",
    "                outputs.append(h)\n",
    "                h = torch.max(h, dim=2)[1]\n",
    "            logits = torch.cat([logits] + outputs, dim=1)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "embed_size = 400\n",
    "hidden_size = 100\n",
    "\n",
    "model = RLSTM(len(vocab), embed_size, hidden_size)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    model = model.cuda()\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 5\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    length = len(train_loader)\n",
    "    \n",
    "    for batch_idx, data in enumerate(train_loader):\n",
    "        inputs, labels = data\n",
    "        inputs = Variable(inputs, volatile=False)\n",
    "        labels = Variable(labels.view(-1), volatile=False)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs.view(-1, len(vocab)), labels)\n",
    "        \n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm(model.parameters(), 0.25)\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.data[0]\n",
    "        \n",
    "        if batch_idx % 100 == 0:\n",
    "            print(batch_idx, train_loss/(batch_idx+1))\n",
    "    \n",
    "    print(\"Epoch \", epoch, \" Train Loss \", train_loss/length)\n",
    "\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    length = len(val_loader)\n",
    "    \n",
    "    for batch_idx, data in enumerate(val_loader):\n",
    "        inputs, labels = data\n",
    "        inputs = Variable(inputs, volatile=False)\n",
    "        labels = Variable(labels.view(-1), volatile=False)\n",
    "        \n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs.view(-1, len(vocab)), labels)\n",
    "        \n",
    "        val_loss += loss.data[0]\n",
    "    \n",
    "    print(\"Epoch \", epoch, \" Val Loss \", val_loss/length)\n",
    "    torch.save(model.state_dict(), str(epoch)+'-model.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = np.load('../fixtures/generation.npy')\n",
    "forward = 20\n",
    "\n",
    "model.load_state_dict(torch.load('model-cpu.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 1, 33278])\n",
      "[[ 1417 21579 31353 30597 32084 15340 18743 15659    76    79 31352    79\n",
      "  22968 15340    76 22968    79    76 31519    73    76    79 21626    79\n",
      "   1424    79 15340    79     6    79    79  7033 15773  4494 20566    79\n",
      "  20481    79 15659    79]\n",
      " [20683 32747 32978 15340    79    79    76 29624  1419    79    79    76\n",
      "     76 13456    79 15340    76    79 21415    72 32427  1509    64    79\n",
      "      1    79    79 15340  7597 15340    79 15659    79 16786  7867  1415\n",
      "     76 20787 22968    76]\n",
      " [25821 15340    79  9820    76    76  7268 22968 24117    76    76    76\n",
      "     79 18119 15659  1419    79    79    76 29798    79    76 22968    79\n",
      "     73 15310 21626    79    79 18957    76    79 28891 17253  1415    76\n",
      "     76  1419 15340    76]\n",
      " [28808    85 14860 15773    76    79    76 17194    76 32084 24697    76\n",
      "  31467    79 25949    79    79 15340 13647 12685    79    76    76    76\n",
      "  19874 31352 15340    76 29004  5768 21626    76 25871  1415 24118    79\n",
      "  31543    79    79    79]\n",
      " [16753    79 32987    76    76     6    76 32084  1424    72 14686    79\n",
      "  23616    79    76    76 22598    76 16464 28636 15340 14146    79 22968\n",
      "  15340 15340  7042    72 15910    76  3919    79  8261 15340    79  1419\n",
      "  32978    79    79    72]\n",
      " [22968 21626 29025    76 15898    79    76 25871 31543    72    76 22035\n",
      "     79    79 19267    76  1414 15659  1414 21454    72 23592    76    72\n",
      "  22968  1424 28866    76    76 12793    79    72    76 14050 32846 25949\n",
      "  25871 15340 24693    73]\n",
      " [16903 18725 19114 28010    79    76  1419    76    76    76    76    72\n",
      "     76 23592 27382  1240    76    76    79  1424    76 31543    79  1419\n",
      "   6784    79 11028  1424    79 15773    79    76    79 22968    72    73\n",
      "  25821  1419    79    76]\n",
      " [ 2103 23592 22668  1419 21626 26118    72  1419    76    76 21387    72\n",
      "     79    79 25821    76 25765    76    79    79    72 22213    76 26712\n",
      "  19779 31352 25723 18620 20531  3992 24723 32747    72    79 31543 15340\n",
      "     76    79 15340    76]\n",
      " [22968    76    79    76 30540 15340 31543    79    72 20500 17080  2514\n",
      "     79 31353 17470  1419  5796 21415    76    79    76  1424    76    79\n",
      "     72 25821    79 11603    76    76 21415    79  2524    79    76    76\n",
      "     79    76  1419    76]\n",
      " [32846    85    76    79    79    76 17173 21415    79 31543    79  5169\n",
      "   1415    79 33246 31543  1419    72 15773 33035    76    79 25821    76\n",
      "     72    76  1424    79    79    76    79 11755    72    72 22968    76\n",
      "  15340  3565    76    79]\n",
      " [   79    76    72 33245  1424 32295    79    79    79    79  1419 14270\n",
      "     79    79    76 15104    76    79 15340  1419 16165    76  1424    79\n",
      "  29591    76 16935    79 22968 15340 22968    76 22968 17682    79 21134\n",
      "   1419    79    76 15340]\n",
      " [   76    76 32978 15340    76 15340 16181  3609    76    79    76    79\n",
      "  27262 13757    79 15340    79 15773    76 25647 26767    79  1558    79\n",
      "  16605    76    76 15773  1415    76    79    76    79    76    79 15340\n",
      "     76    76    79    72]]\n"
     ]
    }
   ],
   "source": [
    "input = torch.from_numpy(inp).long()\n",
    "if torch.cuda.is_available():\n",
    "    input = input.cuda()\n",
    "\n",
    "input = Variable(input)\n",
    "model.eval()\n",
    "logits = model(input, forward=20, stochastic=True)\n",
    "classes = torch.max(logits, dim=2)[1].data.cpu().numpy()\n",
    "print(classes[20:40])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load('4-model.pkl'))\n",
    "torch.save(model.cpu().state_dict(), '4-model-cpu.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
